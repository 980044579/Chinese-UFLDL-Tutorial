# 使用卷积进行特征提取（Feature Extraction Using Convolution）  
##  
## 概览（Overview）  

在之前的练习中，练习问题涉及到的图片其分辨率都偏低，例如小图像修补程序和小图像的手写数字识别。而在本节中，我们将会开发一种方法，它能够扩展先前学到的方法在更实际的大图像数据集上。  

## 全连接网络（Fully Connected Networks）  

在稀疏编码器中，一种设计选择是我们先前已经实现的“全连接”，即所有的隐含层单元与所有输入单元完全连接起来。在我们先前的练习中，使用的是相对较小的图像（例如，在稀疏编码的任务中 8x8 像素大小的图像，MNIST数据集中 28x28 像素大小的图像），这种“全连接”方式的特种学习在对整个图像上计算是可行的。然而，对于更大图像（例如， 96x96 像素大小的图像）的学习来说，可能会因为特征学习会在整个图像上进行（全连接网络），其计算代价是很大的——您要有 $10^4$个输入单元，假设您想学习 100 个特征，您就会对 $10^6$ 个参数按照顺序进行学习。相较于 28x28 像素大小的图像，在前向和反向传播的计算上也会慢大约 $10^2$ 倍。  

## 局部连接网络（Locally Connected Networks）  

这个问题的一种简单解决方案是限制隐含单元与输入单元的连接数目，也就是说，只允许隐含单元连接输入单元中的一个小的子集。具体而言，每个隐藏单元将连接到输入像素中的一个小的连续区域。（对于不同于图像的输入形式，也有一种自然的方式来选择从输入单元到一个隐含单元的“连续组”，例如，对于音频，一个隐藏单元可能被连接到一个与之特定时间跨度对应的音频剪辑的输入单元上。）  

局部连接网络的这一想法也借鉴了在生物学上早期视觉系统的观点。具体而言，视觉皮层的神经元有着局部感受区域（即，它们只会对某一位置的刺激做出反应）。  

## 卷积（Convolutions）  

自然世界中的图像有着“固定不变”的属性，这也意味这图像的某一部分的数据和另一部分的数据是一样的。这表明，我们在一张图片上某部分的特征也可应用到该图片的其它部分，并且我们可以基于这一观点——使用不同的特征，应用到局部数据一样但不同的位置上。  

更确切地说，从大的图像上随机地抽样小图片（比方说 8x8 大小的图片）做特征学习，我们可以将这个8x8大小的特征检测器应用到这幅图片的任何地方。具体而言，我们可以把学到的 8x8 特征，通过将它们与更大图片“卷”起来的方式，在同一张图片上获得在每个位置处不同的特征激活值。  

讲个具体的例子，假设您已经从 96x96 大小的图片上做了8x8大小的抽样的特征学习。再进一步假设，这一过程是通过有着100个隐含单元的自动编码器完成的。为了获得卷积特征（即 96x96 大小的图片上每 8x8 大小范围的特征，这个 8x8 区域是从$(1,1), (2,2), ...(89,89)$ ），您将会提取 8x8 大小的图片，通过您训练的稀疏自动编码器来获取特征激活。这将会产生100组的 89x89 大小的卷积特征。  

<center><img src="./images/Convolution_schematic.gif" /></center>  

正式地说，有一些大小为 $r \times c$ 的图片 $x_{large}$ ，我们首先通过这些图片进行抽样，抽样出大小为 $a\times b$ 的小图片 $x_{small}$ ，利用这些小图片训练一个学习 $k$ 个特征的稀疏自动编码器，这个学习过程是通过给出的从可见单元（译者注：原文中是$visible units$，推测是输入单元）到隐含单元的权重 $W^{(1)}$ 和偏置 $b^{(1)}$，计算 $f = \sigma(W^{(1)}x_{small} + b^{(1)})$ （其中， $\sigma$ 是 S 型函数）。对从大图片抽样出的每个大小为 $a\times b$ 的小图片 $x_{s}$ ，计算该小图片的 $f_s = \sigma(W^{(1)}x_s + b^{(1)})$ ，将这一张大图上的小图计算完，得出这张大图片的 $f_{convolved}$，这个卷积特征是一个规模为 $k \times (r - a + 1) \times (c - b + 1)$ 的数组。  

在下一节中，我们将进一步介绍如何将这些特征“池化”到一起，以获得用于分类的更好特征。